{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7d19e9ba-beaa-471c-bbdd-e085778c7ca4",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Second Book Blog\"\n",
    "description: \"Updating my analysis on my book on AI\"\n",
    "author: \"Emily\"\n",
    "date: \"10/22/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Learning\n",
    "  - Book\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9cee40-ca85-4141-8d97-ea667fdcbceb",
   "metadata": {},
   "source": [
    "![](book.jpg)\n",
    "\n",
    "# Update as of Oct. 21\n",
    "As you know, I am reading *Unmasking AI* by Dr. Joy Buolamwini. I am currently on Chapter 13, page 157, and while I really enjoy Dr. Buolamwini’s analysis of the intersection between artificial intelligence and race, I’m starting to find the book somewhat repetitive at this point. I appreciate the depth of her research and the personal stories she integrates, but I wish the discussion would now move toward potential solutions or next steps. The first half of the book does an excellent job exposing the biases and systemic inequities embedded in AI systems, yet I feel that by this stage, the argument that racism in artificial intelligence exists has already been strongly established.\n",
    "\n",
    "I want to hear more about what can actually be done — what policies, frameworks, or forms of accountability could meaningfully address these issues. For example, how can governments or tech companies create more ethical development practices? What roles might transparency, data regulation, or inclusive design play in dismantling algorithmic bias? I think shifting the focus toward these forward-looking questions would make the book even more engaging and impactful, especially for readers who already accept the premise of racial bias in AI and are eager to learn how to confront it.\n",
    "\n",
    "I know that Dr. Joy Buolamwini discussed how she attended the Conference on Fairness, Accountability, and Transparency in AI. She gave a presentation on her findings about her \"Gender Shades\" paper that incorporated the Fitzpatrick classification systems. She mentions that the representatives from corporatinos kept nervously asking her what her findings were before the presentation and as well as her findings' novelty statuses. They shifted nervously when Dr. Buolamwini found that AI could not distinguish gender as accurately on different races.\n",
    "\n",
    "# Questions and Answers\n",
    "\n",
    "I will be answering questions that I posed in the previous blog post about my book. These are the questions I posed:\n",
    "\n",
    "1) How can AI developers practically ensure that their training datasets are diverse and representative of all racial and ethnic groups?\n",
    "2) How does Dr. Buolamwini approach intersectionality in AI beyond race—does she consider age, disability, or socioeconomic status in her research?\n",
    "3) Will her future work tackle the ethical challenges posed by emerging AI technologies like deepfakes, predictive policing, or AI-driven health diagnostics?\n",
    "4) Could AI ever be completely unbiased, or will it always reflect the society that trains it?\n",
    "5) How does Dr. Buolamwini respond to critics who argue that focusing on bias slows technological progress?\n",
    "\n",
    "The Answers to these Questions:\n",
    "1) Dr. Buolamwini emphasizes that diversity in training datasets must go beyond token inclusion. Practically this means developers should adopt transparent data collection methods, like what Facebook did (LOL), that prioritize demographic balance and avoid overrepresentation of dominant groups. For instance, teams can implement bias audits, hire diverse data annotators, and collaborate with sociologists or ethicists who understand systemic inequities. Buolamwini’s “Gender Shades” study itself exemplifies this: she created a dataset with balanced representation across skin tones and genders, proving that existing commercial datasets were overwhelmingly biased toward lighter-skinned males. Her work suggests that the key is not just collecting more data, but collecting better data — data that reflects the full spectrum of human diversity.\n",
    "2) While *Unmasking AI* focuses most heavily on race and gender, Buolamwini's broader framework of \"coded gaze\" invites consideration of intersectionality beyond these categories. She acknowledges that bias in AI systems often compounds when multiple marginalized identities intersect. Although she doesn't go into deep empiricial analysis of age, disability, or socioeconomic status in this book, her framing leaves space for these spaces to be explored in future work.\n",
    "3) based on her current trajectory and public advocacy, it's likely that Dr. Buolamwini will continue expanding her research to address these evolving ethical challenges. Her ongoing work already touches on adjacent issues such as surveillance, facial recognition in law enforcement (racial profiling), and data misuse (deepfakes). Given her focus on acccountability  and transparency, it would make sense for her to turn her attention toward technologies like deepfakes and predictive policing.\n",
    "4) Dr. Buolamwini makes it clear that complete neutrality in AI is likely impossible because data is always a product of human society and humans are inherently baised. However, that doesn't mean fairness is unattainable or that the fight for fairness and social justice should be stopped. She argues that awareness, intentional design, and transparency can mitigate bias even if they can't eliminate it entirely. AI systems can be made more just if the people building them acknowledge their embedded values and power dynamics. In other words, rather than chasing an impossible \"pure objectivity,\" Buolamwini calls for ethical AI taht reflects conscious choices about whose values are represented and whose are excluded.\n",
    "5) Dr. Buolamwini challenges this argument by redefining what progress truly means. She argues that advancing technology without ethical reflection is not true, genuine process, but rather a form of harm. Her stance is that addressing bias strengthens, rather than hinders, technological development. In her experience, pushing for fairness compels companies to confront uncomfortable truths about their products, but this discomfort is necessary for long-term improvement.\n",
    "\n",
    "# New Questions for Next Blog\n",
    "1) Why might Dr. Buolamwini have chosen to focus so heavily on proving that racial bias in AI exists, rather than moving quickly toward solutions?\n",
    "2) How does Buolamwini’s “Gender Shades” study challenge the credibility of major tech companies and their approach to AI ethics?\n",
    "3) What does the corporate reaction to Buolamwini’s findings at the Conference on Fairness, Accountability, and Transparency reveal about the tech industry’s priorities and discomfort with criticism?\n",
    "4) How can data transparency laws empower individuals who are misclassified or harmed by biased algorithms?\n",
    "5) What types of government regulation or corporate accountability structures could effectively address racial bias in AI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e5e8b-9bca-4d23-ae29-ec50bdbf8110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
